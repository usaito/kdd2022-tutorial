{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPE Experiment with Open Bandit Dataset\n",
    "---\n",
    "This notebook provides an example implementation of conducting OPE of Bernoulli Thompson Sampling (BernoulliTS) as an evaluation policy using some OPE estimators and logged bandit data generated by running the Random policy (logging policy) on the ZOZOTOWN platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import OpenBanditDataset\n",
    "from obp.policy import BernoulliTS\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation, \n",
    "    RegressionModel,\n",
    "    InverseProbabilityWeighting as IPS,\n",
    "    DirectMethod as DM,\n",
    "    DoublyRobust as DR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.4\n"
     ]
    }
   ],
   "source": [
    "# obp version\n",
    "print(obp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Data Loading and Preprocessing\n",
    "\n",
    "`obp.dataset.OpenBanditDataset` is an easy-to-use data loader for Open Bandit Dataset. \n",
    "\n",
    "It takes logging policy ('bts' or 'random') and campaign ('all', 'men', or 'women') as inputs and provides dataset preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:obp.dataset.real:When `data_path` is not given, this class downloads the small-sized version of Open Bandit Dataset.\n"
     ]
    }
   ],
   "source": [
    "# When `data_path` is not given, this class downloads the small-sized version of Open Bandit Dataset.\n",
    "dataset = OpenBanditDataset(behavior_policy='random', campaign='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_rounds', 'n_actions', 'action', 'position', 'reward', 'pscore', 'context', 'action_context'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain logged bandit data generated by logging policy\n",
    "bandit_data = dataset.obtain_batch_bandit_feedback()\n",
    "\n",
    "# `bandit_data` is a dictionary storing logged bandit feedback\n",
    "bandit_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's see some properties of the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'obd'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of the dataset is 'obd' (open bandit dataset)\n",
    "dataset.dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of actions of the \"All\" campaign is 80\n",
    "dataset.n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small example dataset has only 10,000 samples\n",
    "dataset.n_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default context (feature) engineering creates context vector with 20 dimensions\n",
    "dataset.dim_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ZOZOTOWN recommendation interface has three positions\n",
    "# (please see https://github.com/st-tech/zr-obp/blob/master/images/recommended_fashion_items.png)\n",
    "dataset.len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Production Policy Replication\n",
    "\n",
    "After preparing the dataset, we now replicate the BernoulliTS policy implemented on the ZOZOTOWN recommendation interface during the data collection period.\n",
    "\n",
    "Here, we use `obp.policy.BernoulliTS` as an evaluation policy. \n",
    "By activating its `is_zozotown_prior` argument, we can replicate (the policy parameters of) BernoulliTS used in ZOZOTOWN production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_policy = BernoulliTS(\n",
    "    n_actions=dataset.n_actions, # number of actions; |A|\n",
    "    len_list=dataset.len_list, # number of items in a recommendation list; K\n",
    "    is_zozotown_prior=True, # replicate the BernoulliTS policy in the ZOZOTOWN production\n",
    "    campaign=\"all\",\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the action choice probabilities of the evaluation policy via Monte Carlo simulation\n",
    "action_dist = evaluation_policy.compute_batch_action_dist(\n",
    "    n_sim=100000, n_rounds=bandit_data[\"n_rounds\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.01078, 0.00931, 0.00917],\n",
       "        [0.00167, 0.00077, 0.00076],\n",
       "        [0.0058 , 0.00614, 0.00631],\n",
       "        ...,\n",
       "        [0.0008 , 0.00087, 0.00071],\n",
       "        [0.00689, 0.00724, 0.00755],\n",
       "        [0.0582 , 0.07603, 0.07998]],\n",
       "\n",
       "       [[0.01078, 0.00931, 0.00917],\n",
       "        [0.00167, 0.00077, 0.00076],\n",
       "        [0.0058 , 0.00614, 0.00631],\n",
       "        ...,\n",
       "        [0.0008 , 0.00087, 0.00071],\n",
       "        [0.00689, 0.00724, 0.00755],\n",
       "        [0.0582 , 0.07603, 0.07998]],\n",
       "\n",
       "       [[0.01078, 0.00931, 0.00917],\n",
       "        [0.00167, 0.00077, 0.00076],\n",
       "        [0.0058 , 0.00614, 0.00631],\n",
       "        ...,\n",
       "        [0.0008 , 0.00087, 0.00071],\n",
       "        [0.00689, 0.00724, 0.00755],\n",
       "        [0.0582 , 0.07603, 0.07998]],\n",
       "\n",
       "       [[0.01078, 0.00931, 0.00917],\n",
       "        [0.00167, 0.00077, 0.00076],\n",
       "        [0.0058 , 0.00614, 0.00631],\n",
       "        ...,\n",
       "        [0.0008 , 0.00087, 0.00071],\n",
       "        [0.00689, 0.00724, 0.00755],\n",
       "        [0.0582 , 0.07603, 0.07998]],\n",
       "\n",
       "       [[0.01078, 0.00931, 0.00917],\n",
       "        [0.00167, 0.00077, 0.00076],\n",
       "        [0.0058 , 0.00614, 0.00631],\n",
       "        ...,\n",
       "        [0.0008 , 0.00087, 0.00071],\n",
       "        [0.00689, 0.00724, 0.00755],\n",
       "        [0.0582 , 0.07603, 0.07998]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# action_dist is an array of shape (n_rounds, n_actions, len_list) \n",
    "# representing the distribution over actions implied by the evaluation policy\n",
    "action_dist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (3) Off-Policy Evaluation (OPE)\n",
    "\n",
    "The next step is **OPE**, which estimates the performance of new decision making policies using only log data generated by logging policies. \n",
    "\n",
    "Here, we use\n",
    "- **Inverse Propensity Score (IPS)**\n",
    "- **DirectMethod (DM)**\n",
    "- **Doubly Robust (DR)**\n",
    "\n",
    "to estimate the performance of Bernoulli TS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-1) obtain a reward estimator\n",
    "`obp.ope.RegressionModel` simplifies the process of reward modeling\n",
    "\n",
    "$r(x,a) = \\mathbb{E} [r \\mid x, a] \\approx \\hat{r}(x,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obp.ope.RegressionModel\n",
    "regression_model = RegressionModel(\n",
    "    n_actions=dataset.n_actions, # number of actions; |A|\n",
    "    len_list=dataset.len_list, # number of items in a recommendation list; K\n",
    "    base_model=LogisticRegression(C=100, max_iter=10000, random_state=12345), # any sklearn classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_rewards = regression_model.fit_predict(\n",
    "    context=bandit_data[\"context\"],\n",
    "    action=bandit_data[\"action\"],\n",
    "    reward=bandit_data[\"reward\"],\n",
    "    position=bandit_data[\"position\"],\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.58148566e-04, 1.68739271e-02, 1.59238728e-04, ...,\n",
       "        1.62785786e-04, 1.58892825e-04, 1.39337751e-04],\n",
       "       [9.43487804e-05, 1.01350576e-02, 9.49991949e-05, ...,\n",
       "        9.71154498e-05, 9.47928215e-05, 8.31259330e-05],\n",
       "       [9.69432542e-08, 1.05192815e-05, 9.76116177e-08, ...,\n",
       "        9.97862793e-08, 9.73995491e-08, 8.54108347e-08],\n",
       "       ...,\n",
       "       [1.49986944e-04, 1.60169285e-02, 1.51020855e-04, ...,\n",
       "        1.54384887e-04, 1.50692800e-04, 1.32146777e-04],\n",
       "       [3.99016414e-04, 4.15165918e-02, 4.01766279e-04, ...,\n",
       "        4.10713441e-04, 4.00893762e-04, 3.51565900e-04],\n",
       "       [3.27203945e-04, 3.42986101e-02, 3.29459070e-04, ...,\n",
       "        3.36796524e-04, 3.28743530e-04, 2.88290813e-04]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_rewards[:, :, 0] # \\hat{r}(x,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-2) OPE\n",
    "`obp.ope.OffPolicyEvaluation` simplifies the OPE process\n",
    "\n",
    "$V(\\pi_e) \\approx \\hat{V} (\\pi_e; \\mathcal{D}_0, \\theta)$\n",
    "\n",
    "Here we use DM, IPS, and DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ope = OffPolicyEvaluation(\n",
    "    bandit_feedback=bandit_data, # bandit data\n",
    "    ope_estimators=[\n",
    "        IPS(estimator_name=\"IPS\"), \n",
    "        DM(estimator_name=\"DM\"), \n",
    "        DR(estimator_name=\"DR\"),\n",
    "    ] # estimators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_policy_value = ope.estimate_policy_values(\n",
    "    action_dist=action_dist, # \\pi_e(a|x)\n",
    "    estimated_rewards_by_reg_model=estimated_rewards, # \\hat{r}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IPS': 0.00455288, 'DM': 0.004729396841185728, 'DR': 0.0047693518747239025}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPE results given by the three estimators\n",
    "estimated_policy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (4) Evaluation of OPE\n",
    "\n",
    "Our final step is the **evaluation of OPE**, which evaluates the OPE performance (estimation accuracy) of the OPE estimators.\n",
    "\n",
    "Specifically, we evaluate the accuracy of the estimators by comparing their estimation with the ground-truth policy value estimated via the on-policy estimation from Open Bandit Dataset.\n",
    "\n",
    "This type evaluation of OPE is possible, because Open Bandit Dataset contains a set of *multiple* different logged bandit datasets collected by running different policies on the same platform at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1) Approximate the Ground-truth Policy Value\n",
    "$V(\\pi) \\approx \\frac{1}{|\\mathcal{D}_{te}|} \\sum_{i=1}^{|\\mathcal{D}_{te}|} r_i, $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:obp.dataset.real:When `data_path` is not given, this class downloads the small-sized version of Open Bandit Dataset.\n"
     ]
    }
   ],
   "source": [
    "# we first approximate the ground-truth policy value of the evaluation policy\n",
    "# by averaging the factual (observed) rewards contained in the dataset (on-policy estimation)\n",
    "policy_value_bts = OpenBanditDataset.calc_on_policy_policy_value_estimate(\n",
    "    behavior_policy='bts', campaign='all'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2) Evaluation of OPE\n",
    "Now, let's evaluate the OPE performance (estimation accuracy) of the three estimators \n",
    "\n",
    "$SE (\\hat{V}; \\mathcal{D}_0) := \\left( V(\\pi_e) - \\hat{V} (\\pi_e; \\mathcal{D}_0, \\theta) \\right)^2$,     (squared error of $\\hat{V}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_errors = ope.evaluate_performance_of_estimators(\n",
    "    ground_truth_policy_value=policy_value_bts,\n",
    "    action_dist=action_dist,\n",
    "    estimated_rewards_by_reg_model=estimated_rewards,\n",
    "    metric=\"se\", # squared error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IPS': 1.245242943999999e-07,\n",
       " 'DM': 2.8026101545742736e-07,\n",
       " 'DR': 3.241615572516226e-07}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_errors # IPS is the most accurate followed by DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate the above process several times and calculate the following MSE as an accuracy metric of an estimator\n",
    "\n",
    "$MSE (\\hat{V}) := T^{-1} \\sum_{t=1}^T SE (\\hat{V}; \\mathcal{D}_0^{(t)}) $\n",
    "\n",
    "where $\\mathcal{D}_0^{(t)}$ is the synthetic data generated in the $t$-th iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the OPE demonstration here is with the small size example version of our dataset. \n",
    "Please use its full size version (https://research.zozo.com/data.html) to produce more reasonable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d291d498c548e9fa274ccf4ae9326886735d6ad373f47fc6820ad02a5e7fd5a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('3.9.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
