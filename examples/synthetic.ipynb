{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPE/OPL Experiments with Synthetic Bandit Data\n",
    "---\n",
    "This notebook provides an example implementation of conducting OPE of several different evaluation policies with synthetic bandit feedback data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import (\n",
    "    SyntheticBanditDataset,\n",
    "    logistic_reward_function,\n",
    "    linear_behavior_policy,\n",
    ")\n",
    "from obp.policy import IPWLearner\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation, \n",
    "    RegressionModel,\n",
    "    InverseProbabilityWeighting as IPS,\n",
    "    DirectMethod as DM,\n",
    "    DoublyRobust as DR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.4\n"
     ]
    }
   ],
   "source": [
    "# obp version\n",
    "print(obp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Generate Synthetic Data\n",
    "\n",
    "`SyntheticBanditDataset` is an easy-to-use synthetic data generator class implemented in the dataset module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = SyntheticBanditDataset(\n",
    "    n_actions=10, # number of actions; |A|\n",
    "    dim_context=5, # dimension of context vector\n",
    "    reward_function=logistic_reward_function, # expected reward function; r(x,a)\n",
    "    beta=1.0, # temperature parameter to control te logging policy; \\pi_0\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_bandit_data = dataset.obtain_batch_bandit_feedback(n_rounds=10000)\n",
    "test_bandit_data = dataset.obtain_batch_bandit_feedback(n_rounds=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_rounds': 10000,\n",
       " 'n_actions': 10,\n",
       " 'context': array([[-0.20470766,  0.47894334, -0.51943872, -0.5557303 ,  1.96578057],\n",
       "        [ 1.39340583,  0.09290788,  0.28174615,  0.76902257,  1.24643474],\n",
       "        [ 1.00718936, -1.29622111,  0.27499163,  0.22891288,  1.35291684],\n",
       "        ...,\n",
       "        [-1.27028221,  0.80914602, -0.45084222,  0.47179511,  1.89401115],\n",
       "        [-0.68890924,  0.08857502, -0.56359347, -0.41135069,  0.65157486],\n",
       "        [ 0.51204121,  0.65384817, -1.98849253, -2.14429131, -0.34186901]]),\n",
       " 'action_context': array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]),\n",
       " 'action': array([9, 3, 2, ..., 0, 2, 7]),\n",
       " 'position': None,\n",
       " 'reward': array([1, 1, 0, ..., 0, 0, 1]),\n",
       " 'expected_reward': array([[0.81612381, 0.62585527, 0.3867853 , ..., 0.62527072, 0.58635322,\n",
       "         0.38638404],\n",
       "        [0.52901819, 0.30298844, 0.47277431, ..., 0.67711224, 0.55584904,\n",
       "         0.60472268],\n",
       "        [0.47070198, 0.44459997, 0.40016028, ..., 0.71193979, 0.49769816,\n",
       "         0.71876507],\n",
       "        ...,\n",
       "        [0.85229627, 0.60343336, 0.18287765, ..., 0.54555271, 0.77112271,\n",
       "         0.18843358],\n",
       "        [0.78101646, 0.68586084, 0.40700551, ..., 0.45177062, 0.63841605,\n",
       "         0.48128186],\n",
       "        [0.88757249, 0.75954519, 0.82721872, ..., 0.3422384 , 0.33609074,\n",
       "         0.84539856]]),\n",
       " 'pi_b': array([[[0.13284158],\n",
       "         [0.10982506],\n",
       "         [0.08647184],\n",
       "         ...,\n",
       "         [0.10976088],\n",
       "         [0.10557132],\n",
       "         [0.08643715]],\n",
       " \n",
       "        [[0.10165762],\n",
       "         [0.08109171],\n",
       "         [0.09609782],\n",
       "         ...,\n",
       "         [0.11788441],\n",
       "         [0.1044221 ],\n",
       "         [0.10965236]],\n",
       " \n",
       "        [[0.09128276],\n",
       "         [0.08893092],\n",
       "         [0.08506539],\n",
       "         ...,\n",
       "         [0.11618686],\n",
       "         [0.09378061],\n",
       "         [0.11698258]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.13979975],\n",
       "         [0.10900003],\n",
       "         [0.07157834],\n",
       "         ...,\n",
       "         [0.10287015],\n",
       "         [0.12890008],\n",
       "         [0.07197713]],\n",
       " \n",
       "        [[0.12794035],\n",
       "         [0.11632739],\n",
       "         [0.08801904],\n",
       "         ...,\n",
       "         [0.09204875],\n",
       "         [0.11093714],\n",
       "         [0.0948057 ]],\n",
       " \n",
       "        [[0.1309115 ],\n",
       "         [0.11517978],\n",
       "         [0.1232442 ],\n",
       "         ...,\n",
       "         [0.0758826 ],\n",
       "         [0.07541753],\n",
       "         [0.12550525]]]),\n",
       " 'pscore': array([0.08643715, 0.11568983, 0.08506539, ..., 0.13979975, 0.08801904,\n",
       "        0.0758826 ])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_bandit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Train Bandit Policies (OPL)\n",
    "`obp.policy.IPWLearner` is used here (IPS=IPW), but many other choices are possible depending on your research question\n",
    "\n",
    "$ \\hat{\\pi} \\in \\underset{\\pi \\in \\Pi}{\\operatorname{argmax}} \\, \\hat{V}_{I P S}\\left(\\pi ; \\mathcal{D}_{t r}\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipw_learner = IPWLearner(\n",
    "    n_actions=dataset.n_actions, # number of actions; |A|\n",
    "    base_classifier=LogisticRegression(C=100, random_state=12345) # any sklearn classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "ipw_learner.fit(\n",
    "    context=training_bandit_data[\"context\"], # context; x\n",
    "    action=training_bandit_data[\"action\"], # action; a\n",
    "    reward=training_bandit_data[\"reward\"], # reward; r\n",
    "    pscore=training_bandit_data[\"pscore\"], # propensity score; pi_0(a|x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict (action dist = action distribution)\n",
    "action_dist_ipw = ipw_learner.predict(\n",
    "    context=test_bandit_data[\"context\"], # context in the test data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dist_ipw[:, :, 0] # which action to take for each context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (3) Approximate the Ground-truth Policy Value\n",
    "$V(\\pi) \\approx \\frac{1}{|\\mathcal{D}_{te}|} \\sum_{i=1}^{|\\mathcal{D}_{te}|} \\mathbb{E}_{a \\sim \\pi(a|x_i)} [r(x_i, a)], \\; \\, where \\; \\, r(x,a) := \\mathbb{E}_{r \\sim p(r|x,a)} [r]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_value_of_ipw = dataset.calc_ground_truth_policy_value(\n",
    "    expected_reward=test_bandit_data[\"expected_reward\"], # expected reward; r(x,a)\n",
    "    action_dist=action_dist_ipw, # action distribution of IPWLearner\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7932179711565702"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground-truth policy value of `IPWLearner`, which will be compared with OPE estimates\n",
    "policy_value_of_ipw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (4) Off-Policy Evaluation (OPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (4-1) obtain a reward estimator\n",
    "`obp.ope.RegressionModel` simplifies the process of reward modeling\n",
    "\n",
    "$r(x,a) = \\mathbb{E} [r \\mid x, a] \\approx \\hat{r}(x,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = RegressionModel(\n",
    "    n_actions=dataset.n_actions, # number of actions; |A|\n",
    "    base_model=LogisticRegression(C=100, random_state=12345) # any sklearn classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_rewards = regression_model.fit_predict(\n",
    "    context=test_bandit_data[\"context\"], # context; x\n",
    "    action=test_bandit_data[\"action\"], # action; a\n",
    "    reward=test_bandit_data[\"reward\"], # reward; r\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65272354, 0.61704824, 0.586804  , ..., 0.48577246, 0.66897743,\n",
       "        0.61495777],\n",
       "       [0.67466355, 0.63999902, 0.61042298, ..., 0.51034758, 0.69037767,\n",
       "        0.63796033],\n",
       "       [0.6132436 , 0.5761462 , 0.54505335, ..., 0.44349471, 0.63029871,\n",
       "        0.57398661],\n",
       "       ...,\n",
       "       [0.53528662, 0.49684684, 0.46533516, ..., 0.36665906, 0.55327521,\n",
       "        0.49463761],\n",
       "       [0.56154344, 0.52334154, 0.49179191, ..., 0.39161478, 0.57931443,\n",
       "        0.52113652],\n",
       "       [0.6701928 , 0.63530941, 0.60558553, ..., 0.5052746 , 0.68602237,\n",
       "        0.63325939]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_rewards[:, :, 0] # \\hat{r}(x,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2) OPE\n",
    "`obp.ope.OffPolicyEvaluation` simplifies the OPE process\n",
    "\n",
    "$V(\\pi_e) \\approx \\hat{V} (\\pi_e; \\mathcal{D}_0, \\theta)$ \n",
    "\n",
    "Here we use DM, IPS, and DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ope = OffPolicyEvaluation(\n",
    "    bandit_feedback=test_bandit_data, # test data\n",
    "    ope_estimators=[\n",
    "        IPS(estimator_name=\"IPS\"), \n",
    "        DM(estimator_name=\"DM\"), \n",
    "        DR(estimator_name=\"DR\"),\n",
    "    ] # estimators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimated_policy_value = ope.estimate_policy_values(\n",
    "    action_dist=action_dist_ipw, # \\pi_e(a|x)\n",
    "    estimated_rewards_by_reg_model=estimated_rewards, # \\hat{r}(x,a)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IPS': 0.7888273216784351, 'DM': 0.5990759314912072, 'DR': 0.7910361006651566}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPE results given by the three estimators\n",
    "estimated_policy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Evaluation of OPE\n",
    "Now, let's evaluate the OPE performance (estimation accuracy) of the three estimators\n",
    "\n",
    "$SE (\\hat{V}; \\mathcal{D}_0) := \\left( V(\\pi_e) - \\hat{V} (\\pi_e; \\mathcal{D}_0, \\theta) \\right)^2$,     (squared error of $\\hat{V}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_errors = ope.evaluate_performance_of_estimators(\n",
    "    ground_truth_policy_value=policy_value_of_ipw, # V(\\pi_e)\n",
    "    action_dist=action_dist_ipw, # \\pi_e(a|x)\n",
    "    estimated_rewards_by_reg_model=estimated_rewards, # \\hat{q}(x,a)\n",
    "    metric=\"se\", # squared error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IPS': 1.9277802839848613e-05,\n",
       " 'DM': 0.037691131565427395,\n",
       " 'DR': 4.760558841301454e-06}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_errors # DR is the most accurate followed by IPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate the above process several times and calculate the following MSE as an accuracy metric of an estimator\n",
    "\n",
    "$MSE (\\hat{V}) := T^{-1} \\sum_{t=1}^T SE (\\hat{V}; \\mathcal{D}_0^{(t)}) $\n",
    "\n",
    "where $\\mathcal{D}_0^{(t)}$ is the synthetic data generated in the $t$-th iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d291d498c548e9fa274ccf4ae9326886735d6ad373f47fc6820ad02a5e7fd5a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('3.9.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
