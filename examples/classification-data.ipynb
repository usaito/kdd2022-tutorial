{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPE Experiment with Classification Data\n",
    "---\n",
    "This notebook provides an example implementation of conducting OPE of an evaluation policy using/regarding classification data as logged bandit data.\n",
    "It is quite common to conduct OPE experiments using classification data (a type of semi-synthetic experiment). Appendix G of [Farajtabar et al.(2018)](https://arxiv.org/abs/1802.03493) describes how to conduct OPE experiments with classification data in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import MultiClassToBanditReduction\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation, \n",
    "    RegressionModel,\n",
    "    InverseProbabilityWeighting as IPS,\n",
    "    DirectMethod as DM,\n",
    "    DoublyRobust as DR, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.4\n"
     ]
    }
   ],
   "source": [
    "# obp version\n",
    "print(obp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (1) Bandit Reduction\n",
    "`obp.dataset.MultiClassToBanditReduction` is an easy-to-use dataset class for transforming classification data to bandit data.\n",
    "It takes \n",
    "- feature vectors (`X`)\n",
    "- class labels (`y`)\n",
    "- classifier to construct logging policy (`base_classifier_b`) \n",
    "- parameter of logging policy (`alpha_b`) \n",
    "\n",
    "as inputs and generates a bandit dataset that can be used to evaluate the performance of decision making policies (obtained by `off-policy learning`) and OPE estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw digits data\n",
    "# `return_X_y` splits feature vectors and labels\n",
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the raw classification data into a logged bandit dataset\n",
    "# we construct a logging policy using Logistic Regression and parameter alpha_b\n",
    "# given a pair of a feature vector and a label (x, c), create a pair of a context vector and reward (x, r)\n",
    "# where r = 1 if the output of the logging policy is equal to c and r = 0 otherwise\n",
    "dataset = MultiClassToBanditReduction(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    base_classifier_b=LogisticRegression(max_iter=10000, random_state=12345),\n",
    "    alpha_b=0.8,\n",
    "    dataset_name=\"digits\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the original data into training and evaluation sets\n",
    "dataset.split_train_eval(eval_size=0.7, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_actions': 10,\n",
       " 'n_rounds': 1258,\n",
       " 'context': array([[ 0.,  0.,  0., ..., 16.,  1.,  0.],\n",
       "        [ 0.,  0.,  7., ..., 16.,  3.,  0.],\n",
       "        [ 0.,  0., 12., ...,  8.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  1., 13., ...,  8., 11.,  1.],\n",
       "        [ 0.,  0., 15., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  4., ..., 15.,  3.,  0.]]),\n",
       " 'action': array([6, 8, 5, ..., 2, 5, 9]),\n",
       " 'reward': array([1., 1., 1., ..., 1., 1., 1.]),\n",
       " 'position': None,\n",
       " 'pscore': array([0.82, 0.82, 0.82, ..., 0.82, 0.82, 0.82])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain logged bandit data generated by logging policy\n",
    "bandit_data = dataset.obtain_batch_bandit_feedback(random_state=12345)\n",
    "\n",
    "# `bandit_data` is a dictionary storing logged bandit data\n",
    "bandit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Off-Policy Learning\n",
    "After generating logged bandit data, we now obtain an evaluation policy using the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain action choice probabilities of an evaluation policy\n",
    "# here we construct an evaluation policy using Random Forest and parameter alpha_e\n",
    "action_dist = dataset.obtain_action_dist_by_eval_policy(\n",
    "    base_classifier_e=RandomForestClassifier(random_state=12345),\n",
    "    alpha_e=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.91, 0.01],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
       "       ...,\n",
       "       [0.01, 0.01, 0.91, ..., 0.01, 0.01, 0.01],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.91]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which action to take for each context (a probability distribution over actions)\n",
    "action_dist[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (3) Off-Policy Evaluation (OPE)\n",
    "OPE aims at estimating the performance of evaluation policies based on their action choice probabilities.\n",
    "\n",
    "Here, we evaluate/compare the OPE performance (estimation accuracy) of \n",
    "- **Inverse Propensity Score (IPS)**\n",
    "- **DirectMethod (DM)**\n",
    "- **Doubly Robust (DR)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-1) obtain a reward estimator\n",
    "`obp.ope.RegressionModel` simplifies the process of reward modeling\n",
    "\n",
    "$r(x,a) = \\mathbb{E} [r \\mid x, a] \\approx \\hat{r}(x,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = RegressionModel(\n",
    "    n_actions=dataset.n_actions, # number of actions; |A|\n",
    "    base_model=LogisticRegression(C=100, max_iter=10000, random_state=12345), # any sklearn classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_rewards = regression_model.fit_predict(\n",
    "    context=bandit_data[\"context\"],\n",
    "    action=bandit_data[\"action\"],\n",
    "    reward=bandit_data[\"reward\"],\n",
    "    position=bandit_data[\"position\"],\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91281795, 0.86737252, 0.91463069, ..., 0.81168002, 0.89845427,\n",
       "        0.9358936 ],\n",
       "       [0.88903207, 0.83345002, 0.89128047, ..., 0.76733391, 0.87130206,\n",
       "        0.91783677],\n",
       "       [0.74513876, 0.64616803, 0.74948112, ..., 0.54618716, 0.71186922,\n",
       "        0.80301895],\n",
       "       ...,\n",
       "       [0.81793626, 0.73726738, 0.82133568, ..., 0.64904708, 0.79151076,\n",
       "        0.86233814],\n",
       "       [0.96992863, 0.95271105, 0.97059214, ..., 0.92995996, 0.96460941,\n",
       "        0.97824823],\n",
       "       [0.59247763, 0.47591952, 0.59801785, ..., 0.37440688, 0.55128051,\n",
       "        0.66965761]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_rewards[:, :, 0] # \\hat{r}(x,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-2) OPE\n",
    "`obp.ope.OffPolicyEvaluation` simplifies the OPE process\n",
    "\n",
    "$V(\\pi_e) \\approx \\hat{V} (\\pi_e; \\mathcal{D}_0, \\theta)$ \n",
    "\n",
    "Here, we use DM, IPS, and DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ope = OffPolicyEvaluation(\n",
    "    bandit_feedback=bandit_data, # bandit data\n",
    "    ope_estimators=[\n",
    "        IPS(estimator_name=\"IPS\"), \n",
    "        DM(estimator_name=\"DM\"), \n",
    "        DR(estimator_name=\"DR\"),\n",
    "    ] # estimators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimated_policy_value = ope.estimate_policy_values(\n",
    "    action_dist=action_dist, # \\pi_e(a|x)\n",
    "    estimated_rewards_by_reg_model=estimated_rewards, # \\hat{r}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IPS': 0.891155143665904, 'DM': 0.788981343944812, 'DR': 0.8752874797701774}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPE results given by the three estimators\n",
    "estimated_policy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (4) Evaluation of OPE estimators\n",
    "Our final step is **the evaluation of OPE**, which evaluates and compares the estimation accuracy of OPE estimators.\n",
    "\n",
    "With the multi-class classification data, we can approximate the ground-truth policy value of the evaluation policy. \n",
    "Therefore, we can compare the estimated policy values with the ground-truth to evaluate OPE estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4-1) Approximate the Ground-truth Policy Value\n",
    "$V(\\pi) \\approx \\frac{1}{|\\mathcal{D}_{te}|} \\sum_{i=1}^{|\\mathcal{D}_{te}|} \\mathbb{E}_{a \\sim \\pi(a|x_i)} [r(x_i, a)], \\; \\, where \\; \\, r(x,a) := \\mathbb{E}_{r \\sim p(r|x,a)} [r]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8770906200317964"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# approximate the ground-truth performance of the evaluation policy\n",
    "true_policy_value = dataset.calc_ground_truth_policy_value(action_dist=action_dist)\n",
    "\n",
    "true_policy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2) Evaluation of OPE\n",
    "Now, let's evaluate the OPE performance (estimation accuracy) of the three estimators \n",
    "\n",
    "$SE (\\hat{V}; \\mathcal{D}_0) := \\left( V(\\pi_e) - \\hat{V} (\\pi_e; \\mathcal{D}_0, \\theta) \\right)^2$,     (squared error of $\\hat{V}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_errors = ope.evaluate_performance_of_estimators(\n",
    "    ground_truth_policy_value=true_policy_value,\n",
    "    action_dist=action_dist,\n",
    "    estimated_rewards_by_reg_model=estimated_rewards,\n",
    "    metric=\"se\", # squared error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IPS': 0.00019781082505437113,\n",
       " 'DM': 0.007763244532572437,\n",
       " 'DR': 3.251314803071382e-06}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_errors # DR is the most accurate followed by IPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate the above process several times and calculate the following MSE as an accuracy metric of an estimator\n",
    "\n",
    "$MSE (\\hat{V}) := T^{-1} \\sum_{t=1}^T SE (\\hat{V}; \\mathcal{D}_0^{(t)}) $\n",
    "\n",
    "where $\\mathcal{D}_0^{(t)}$ is the synthetic data in the $t$-th iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d291d498c548e9fa274ccf4ae9326886735d6ad373f47fc6820ad02a5e7fd5a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('3.9.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
