{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# OPE Experiment with Classificatoin Data\n",
    "---\n",
    "This notebook provides an example of conducting OPE of an evaluation policy using classification data as logged bandit data.\n",
    "It is quite common to conduct OPE experiments using classification data. Appendix G of [Farajtabar et al.(2018)](https://arxiv.org/abs/1802.03493) describes how to conduct OPE experiments with classification data in detail."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import MultiClassToBanditReduction\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation, \n",
    "    RegressionModel,\n",
    "    InverseProbabilityWeighting as IPS,\n",
    "    DirectMethod as DM,\n",
    "    DoublyRobust as DR, \n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# obp version\n",
    "print(obp.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5.1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (1) Bandit Reduction\n",
    "`obp.dataset.MultiClassToBanditReduction` is an easy-to-use for transforming classification data to bandit data.\n",
    "It takes \n",
    "- feature vectors (`X`)\n",
    "- class labels (`y`)\n",
    "- classifier to construct behavior policy (`base_classifier_b`) \n",
    "- paramter of behavior policy (`alpha_b`) \n",
    "\n",
    "as its inputs and generates a bandit data that can be used to evaluate the performance of decision making policies (obtained by `off-policy learning`) and OPE estimators."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# load raw digits data\n",
    "# `return_X_y` splits feature vectors and labels, instead of returning a Bunch object\n",
    "X, y = load_digits(return_X_y=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# convert the raw classification data into a logged bandit dataset\n",
    "# we construct a behavior policy using Logistic Regression and parameter alpha_b\n",
    "# given a pair of a feature vector and a label (x, c), create a pair of a context vector and reward (x, r)\n",
    "# where r = 1 if the output of the behavior policy is equal to c and r = 0 otherwise\n",
    "# please refer to https://zr-obp.readthedocs.io/en/latest/_autosummary/obp.dataset.multiclass.html for the details\n",
    "dataset = MultiClassToBanditReduction(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    base_classifier_b=LogisticRegression(max_iter=10000, random_state=12345),\n",
    "    alpha_b=0.8,\n",
    "    dataset_name=\"digits\",\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# split the original data into training and evaluation sets\n",
    "dataset.split_train_eval(eval_size=0.7, random_state=12345)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# obtain logged bandit data generated by behavior policy\n",
    "bandit_data = dataset.obtain_batch_bandit_feedback(random_state=12345)\n",
    "\n",
    "# `bandit_data` is a dictionary storing logged bandit feedback\n",
    "bandit_data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'n_actions': 10,\n",
       " 'n_rounds': 1258,\n",
       " 'context': array([[ 0.,  0.,  0., ..., 16.,  1.,  0.],\n",
       "        [ 0.,  0.,  7., ..., 16.,  3.,  0.],\n",
       "        [ 0.,  0., 12., ...,  8.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  1., 13., ...,  8., 11.,  1.],\n",
       "        [ 0.,  0., 15., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  4., ..., 15.,  3.,  0.]]),\n",
       " 'action': array([6, 8, 5, ..., 2, 5, 9]),\n",
       " 'reward': array([1., 1., 1., ..., 1., 1., 1.]),\n",
       " 'position': None,\n",
       " 'pscore': array([0.82, 0.82, 0.82, ..., 0.82, 0.82, 0.82])}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (2) Off-Policy Learning\n",
    "After generating logged bandit data, we now obtain an evaluation policy using the training set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# obtain action choice probabilities by an evaluation policy\n",
    "# we construct an evaluation policy using Random Forest and parameter alpha_e\n",
    "action_dist = dataset.obtain_action_dist_by_eval_policy(\n",
    "    base_classifier_e=RandomForestClassifier(random_state=12345),\n",
    "    alpha_e=0.9,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# which action to take for each context (a probability distribution over actions)\n",
    "action_dist[:, :, 0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.91, 0.01],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
       "       ...,\n",
       "       [0.01, 0.01, 0.91, ..., 0.01, 0.01, 0.01],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.91]])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (3) Off-Policy Evaluation (OPE)\n",
    "OPE attempts to estimate the performance of evaluation policies using their action choice probabilities.\n",
    "\n",
    "Here, we evaluate/compare the OPE performance (estimation accuracy) of \n",
    "- **Inverse Propensity Score (IPS)**\n",
    "- **DirectMethod (DM)**\n",
    "- **Doubly Robust (DR)**"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (3-1) obtain a reward estimator\n",
    "`obp.ope.RegressionModel` simplifies the process of reward modeling\n",
    "\n",
    "$r(x,a) = \\mathbb{E} [r \\mid x, a] \\approx \\hat{r}(x,a)$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "regression_model = RegressionModel(\n",
    "    n_actions=dataset.n_actions, # number of actions; |A|\n",
    "    base_model=LogisticRegression(C=100, max_iter=10000, random_state=12345), # any sklearn classifier\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "estimated_rewards = regression_model.fit_predict(\n",
    "    context=bandit_data[\"context\"],\n",
    "    action=bandit_data[\"action\"],\n",
    "    reward=bandit_data[\"reward\"],\n",
    "    position=bandit_data[\"position\"],\n",
    "    random_state=12345,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "estimated_rewards[:, :, 0] # \\hat{q}(x,a)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.91281795, 0.86737252, 0.91463069, ..., 0.81168002, 0.89845427,\n",
       "        0.9358936 ],\n",
       "       [0.88903207, 0.83345002, 0.89128047, ..., 0.76733391, 0.87130206,\n",
       "        0.91783677],\n",
       "       [0.74513876, 0.64616803, 0.74948112, ..., 0.54618716, 0.71186922,\n",
       "        0.80301895],\n",
       "       ...,\n",
       "       [0.81793626, 0.73726738, 0.82133568, ..., 0.64904708, 0.79151076,\n",
       "        0.86233814],\n",
       "       [0.96992863, 0.95271105, 0.97059214, ..., 0.92995996, 0.96460941,\n",
       "        0.97824823],\n",
       "       [0.59247763, 0.47591952, 0.59801785, ..., 0.37440688, 0.55128051,\n",
       "        0.66965761]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (3-2) OPE\n",
    "`obp.ope.OffPolicyEvaluation` simplifies the OPE process\n",
    "\n",
    "$V(\\pi_e) \\approx \\hat{V} (\\pi_e; \\mathcal{D}_0, \\theta)$ using DM, IPS, and DR"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "ope = OffPolicyEvaluation(\n",
    "    bandit_feedback=bandit_data, # bandit data\n",
    "    ope_estimators=[\n",
    "        IPS(estimator_name=\"IPS\"), \n",
    "        DM(estimator_name=\"DM\"), \n",
    "        DR(estimator_name=\"DR\"),\n",
    "    ] # used estimators\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "estimated_policy_value = ope.estimate_policy_values(\n",
    "    action_dist=action_dist, # \\pi_e(a|x)\n",
    "    estimated_rewards_by_reg_model=estimated_rewards, # \\hat{q}\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# OPE results given by the three estimators\n",
    "estimated_policy_value"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'IPS': 0.891155143665904, 'DM': 0.788981343944812, 'DR': 0.8752874797701774}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (4) Evaluation of OPE estimators\n",
    "Our final step is **the evaluation of OPE**, which evaluates and compares the estimation accuracy of OPE estimators.\n",
    "\n",
    "With the multi-class classification data, we can calculate the ground-truth policy value of the evaluation policy. \n",
    "Therefore, we can compare the policy values estimated by OPE estimators with the ground-turth to evaluate OPE estimators."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (4-1) Approximate the Ground-truth Policy Value\n",
    "$V(\\pi) \\approx \\frac{1}{|\\mathcal{D}_{te}|} \\sum_{i=1}^{|\\mathcal{D}_{te}|} \\mathbb{E}_{a \\sim \\pi(a|x_i)} [r(x_i, a)], \\; \\, where \\; \\, r(x,a) := \\mathbb{E}_{r \\sim p(r|x,a)} [r]$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# calculate the ground-truth performance of the evaluation policy\n",
    "true_policy_value = dataset.calc_ground_truth_policy_value(action_dist=action_dist)\n",
    "\n",
    "true_policy_value"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8770906200317964"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (4-2) Evaluation of OPE\n",
    "Now, let's evaluate the OPE performance (estimation accuracy) of the three estimators \n",
    "\n",
    "$SE (\\hat{V}; \\mathcal{D}_0) := \\left( V(\\pi_e) - \\hat{V} (\\pi_e; \\mathcal{D}_0, \\theta) \\right)^2$,     (squared error of $\\hat{V}$)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "squared_errors = ope.evaluate_performance_of_estimators(\n",
    "    ground_truth_policy_value=true_policy_value,\n",
    "    action_dist=action_dist,\n",
    "    estimated_rewards_by_reg_model=estimated_rewards,\n",
    "    metric=\"se\", # squared error\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "squared_errors # DR is the most accurate "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'IPS': 0.00019781082505437113,\n",
       " 'DM': 0.007763244532572437,\n",
       " 'DR': 3.251314803071382e-06}"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can iterate the above process several times and calculate the following MSE\n",
    "\n",
    "$MSE (\\hat{V}) := T^{-1} \\sum_{t=1}^T SE (\\hat{V}; \\mathcal{D}_0^{(t)}) $\n",
    "\n",
    "where $\\mathcal{D}_0^{(t)}$ is the synthetic data in the $t$-th iteration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('zr-obp': pyenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "64b446a4e17784c2dc3dbe74bf0708929a003fb4822b30671d57ebdef413b716"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}